# -*- coding: utf-8 -*-
"""preprocess.ipynb
Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/drive/10Jjz8zCIYFliOfRi0nLQQgOuuSsOB1y6
**Project 1 group 8**
# **Data Preprocessing Stage**
## **Read data from file**
"""

import os
import codecs
import pandas as pd

def readfile(filename):
    current_path=os.path.abspath(os.curdir)
    file_path=os.path.join(current_path,filename)
    if not os.path.exists(file_path):
        print("error:file not found:"+filename)
        return ""
    f=codecs.open(file_path,"r","utf-8")
    s=f.read()
    f.close()
    return s
test_data=readfile("test-public.txt")
test_data=test_data.splitlines()
print("length of testing data:"+str(len(test_data)))
train_data=readfile("train.txt")
train_data=train_data.splitlines()
print("length of training data:"+str(len(train_data)))

"""## **preprocessing data**
- training data
"""

train_list=[]
for line in train_data:
    # remove \n at the end of each line
    line = line.strip()
    # split IDs by \t
    line_list = line.split("\t")
    train_list.append(line_list)
# print(train_list)


"""- testing data
*remove first line (no data)*
"""
del test_data[0]

test_list=[]

for temp_str in test_data:
    temp=temp_str.split('\t')
    temp_list=[]
    temp_list.append(int(temp[0]))
    temp_list.append(int(temp[1]))
    temp_list.append(int(temp[2]))
    test_list.append(temp_list)
print("preprocessed testing data length:"+str(len(test_list)))

"""**check validity**"""

#print(test_list)
#
""" Similarity Model """

from collections import defaultdict
node_set = set()
follower = defaultdict(set)
follow = defaultdict(set)
# follower = set()
# follow = set()

for instance in train_list:
    source,sink_list = instance[0],instance[1:]
    node_set.add(source)
    follow[source] = set(sink_list)
    for node in sink_list:
        node_set.add(node)
        follower[node].add(source)
    # print(follower)
    # print(follow)

train_dic = {}
for item in train_list:
    train_dic[item[0]] = item[1:]
#
# def common_follow(node1,node2,dict):
#     user1 = (node1,dict)
#     user2 = (node2,dict)
#     return len(follow[user1].intersection(follow[user2]))+len(follower[user1].intersection(follower[user2]))

def Jaccard(node1,node2):
    return float(len(follow[user1].intersection(follow[user2]))+len(follower[user1].intersection(follower[user2]))/(len(follow[node1].union(follow[node2]))+len(follower[node1].union(follower[node2]))))

#
# def get_feature(node1,node2):
#     result = []
#     result.append(Jaccard(node1,node2))
#

Result = []
for ID, source, sink in test_list:
    Result.append([ID, Jaccard(source, sink, train_dic)])



# output
title = ["Id", "Predicted"]
test_pd = pd.DataFrame(columns = title, data = Result)
test_pd.to_csv('/Users/elvis/Desktop/Result.csv', encoding='utf-8')
